{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Mining Assignment 1 \n",
    "# Analysis of Moby Dick Novel and Spell Checker Recommender\n",
    "# Name: Sushmitha Mani, UCF ID: 5016977\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\97150\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the Moby Dick text file \n",
    "with open('C:\\\\Users\\97150\\Documents\\FALL20\\Text Mining\\mobyfile.txt', 'r') as f:\n",
    "    rawfile_moby = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tokens present in the file: 255018\n"
     ]
    }
   ],
   "source": [
    "#Question -1\n",
    "#number of tokens in the text \n",
    "\n",
    "#Defining a function to return the number of tokens present in the text file. Storing the integer value in a variable tokens_moby\n",
    "def no_of_tokens():\n",
    "    return len(nltk.word_tokenize(rawfile_moby))\n",
    "\n",
    "tokens_moby= no_of_tokens()\n",
    "print('The number of tokens present in the file:', tokens_moby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of Unique tokens present in the file: 20754\n"
     ]
    }
   ],
   "source": [
    "#Question1\n",
    "#number of unique tokens in the text file \n",
    "\n",
    "#Defining a function to return the number of unique tokens present in the text file. Storing the integer in a variable uniquetokens_moby\n",
    "def no_of_uniquetokens():\n",
    "    return len(set(nltk.word_tokenize(rawfile_moby)))\n",
    "\n",
    "uniquetokens_moby= no_of_uniquetokens()\n",
    "print('The number of Unique tokens present in the file:', uniquetokens_moby)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\97150\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inorder to use Lemmatization, we need to have wordnet downloaded \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtaining the number of tokens and storing it in the variable moby_tokens. Converting the text into nltk format using the following method \n",
    "moby_tokens = nltk.word_tokenize(rawfile_moby)\n",
    "newtext = nltk.Text(moby_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recalculated unique tokens after Lemmatization:  16899\n"
     ]
    }
   ],
   "source": [
    "#Question 2\n",
    "#Lemmatization of the verbs in the text and recalculating the new tokens and unique tokens\n",
    "#lemmatization is done using the lemmatizer.lemmatize method\n",
    "#the following has been done for the recalculation of new unique tokens in the text file. The new value is stored in a variable. \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemma_uniquetokens():\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w,'v') for w in newtext]\n",
    "    return len(set(lemmatized))\n",
    "\n",
    "ul= lemma_uniquetokens()\n",
    "\n",
    "print('The recalculated unique tokens after Lemmatization: ', ul )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recalculated Tokens after Lemmatization:  255018\n"
     ]
    }
   ],
   "source": [
    "#similar method is followed for tokens.\n",
    "#Lemmatization of the verbs and recalculating the new tokens alone:\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemma_tokens():\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w,'v') for w in newtext]\n",
    "    return len((lemmatized))\n",
    "\n",
    "lt= lemma_tokens()\n",
    "\n",
    "\n",
    "print('The recalculated Tokens after Lemmatization: ', lt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of tokens which are History or HISTORY is:  0.0031370334643044807 %\n"
     ]
    }
   ],
   "source": [
    "#Question 3\n",
    "#Percentage of tokens which are either 'HISTORY' or 'History'\n",
    "#Percentage= [Sum of the tokens which are 'HISTORY' and 'History' / total number of tokens] x100\n",
    "\n",
    "\n",
    "def percentage_history():\n",
    "    return(newtext.vocab()['HISTORY'] + newtext.vocab()['History'])/len(nltk.word_tokenize(rawfile_moby))*100\n",
    "\n",
    "print('The percentage of tokens which are History or HISTORY is: ', percentage_history(), '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 frequently occuring tokens in the moby text are as follows:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(',', 19204),\n",
       " ('the', 13715),\n",
       " ('.', 7308),\n",
       " ('of', 6513),\n",
       " ('and', 6010),\n",
       " ('a', 4545),\n",
       " ('to', 4515),\n",
       " (';', 4173),\n",
       " ('in', 3908),\n",
       " ('that', 2978)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Question 4\n",
    "#To find the most commonly occurring top 10 tokens in the Moby Dick Text \n",
    "#top occuring words are obtained using the FreqDist and most_common methods.\n",
    "\n",
    "def topten():\n",
    "    moby_tokens = nltk.word_tokenize(rawfile_moby)\n",
    "    dist = nltk.FreqDist(moby_tokens)\n",
    "    return dist.most_common(10)\n",
    "\n",
    "print('The top 10 frequently occuring tokens in the moby text are as follows:')\n",
    "topten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary packages for the spell checker \n",
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk.corpus import words\n",
    "from nltk.util import ngrams\n",
    "\n",
    "spellings_actual= words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your value for spelling check:  validrate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correct spelling is:  ['validate']\n"
     ]
    }
   ],
   "source": [
    "#Spell Recommender/Checker \n",
    "\n",
    "#The spell recommender is done using the edit distance or the levenshtein distance \n",
    "#We take the user input and store them in an list called list_user_values\n",
    "#We the calculate the edit distance with comparison to the actual spelling imported from the nltk corpus words package \n",
    "# we take the minimum of the edit distance[1] as the correct spelling and append it in out correct output list\n",
    "\n",
    "list_user_values=[]\n",
    "user_input = input(\"Enter your value for spelling check: \") \n",
    "list_user_values  = user_input.split()\n",
    "\n",
    "for i in list_user_values:\n",
    "        corrected_output = []\n",
    "        calculate_distances = ((edit_distance(i,word), word) for word in spellings_actual)\n",
    "        nearest_value = min(calculate_distances)\n",
    "        corrected_output.append(nearest_value[1])\n",
    "\n",
    "\n",
    "print('The correct spelling is: ', corrected_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
